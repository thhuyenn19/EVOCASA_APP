import os
import re
import time
import asyncio
from concurrent.futures import ThreadPoolExecutor
from dotenv import load_dotenv
from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings
from langchain_community.vectorstores import Chroma
from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationBufferMemory
from langchain.prompts import PromptTemplate
from firebase_util import init_firebase, get_all_documents_as_texts
from firebase_admin import firestore

load_dotenv()
init_firebase()

# Kh·ªüi t·∫°o Firestore client
db = firestore.client()

# Cache categories ƒë·ªÉ tr√°nh g·ªçi Firestore m·ªói l·∫ßn
CATEGORIES_CACHE = {}
CACHE_TIMESTAMP = 0
CACHE_DURATION = 300  # 5 ph√∫t

# Enhanced common responses - th√¢n thi·ªán v√† chi ti·∫øt h∆°n
COMMON_RESPONSES = {
    "hello": "Hello! Welcome to EvoCasa! üòä I'm your furniture consultant, ready to help you find the perfect pieces for your home. Whether you're looking for specific products, pricing information, or need advice on furniture selection, I'm here to assist you. What can I help you with today?",
    "hi": "Hi there! Great to have you at EvoCasa! üè† I'm here to help you explore our premium furniture collection. I can provide product details, pricing, availability, design advice, or help you compare different options. How may I assist you?",
    "hey": "Hey! Welcome to EvoCasa - where comfort meets style! üëã I'm your personal furniture assistant, ready to help you discover amazing pieces for your space. What would you like to know about our collection today?",
    "good morning": "Good morning! What a wonderful day to explore beautiful furniture! ‚òÄÔ∏è I'm here to help you find exactly what you need for your home. Whether it's a specific item or general browsing, I'm at your service. How can I assist you this morning?",
    "good afternoon": "Good afternoon! I hope you're having a great day! üå§Ô∏è I'm here to help you with all your EvoCasa furniture needs - from product information to design suggestions. What can I help you discover today?",
    "good evening": "Good evening! Perfect time to plan your home improvements! üåô I'm here to help you explore our furniture collection and find pieces that will make your space even more beautiful. How may I assist you tonight?",
    "thanks": "You're absolutely welcome! üòä I'm so glad I could help you today. If you have any other questions about our products, need more details, or want to explore other options, please don't hesitate to ask. I'm here whenever you need assistance!",
    "thank you": "It's my pleasure to help you! üåü That's what I'm here for. If you need any additional information, want to see similar products, or have questions about delivery and services, feel free to ask anytime. I'm always ready to assist!",
    "bye": "Thank you so much for visiting EvoCasa today! üëã It was wonderful helping you explore our furniture collection. Don't forget to check back for new arrivals and special offers. Have a fantastic day, and see you again soon!",
    "goodbye": "Goodbye and thank you for choosing EvoCasa! üåü I hope you found everything you were looking for. Remember, I'm always here if you need more information or assistance with your furniture needs. Take care and have a wonderful day!",
    "how are you": "I'm doing fantastic, thank you for asking! üòä I'm excited and ready to help you discover amazing furniture pieces today. EvoCasa has such a wonderful collection, and I love helping customers find exactly what they need for their homes. How are you doing, and what brings you to EvoCasa today?",
    "what's up": "Hello there! I'm here and ready to help you with anything EvoCasa related! üõãÔ∏è Whether you're looking for specific furniture pieces, comparing options, checking prices, or just browsing for inspiration, I'm your go-to assistant. What's on your furniture wishlist today?"
}

# Function to check for common responses - v·∫´n gi·ªØ exact matching
def check_common_response(query):
    query_lower = query.lower().strip()
    
    # Ch·ªâ check exact matches ƒë·ªÉ tr√°nh nh·∫ßm l·∫´n
    if query_lower in COMMON_RESPONSES:
        return COMMON_RESPONSES[query_lower]
    
    # Ch·ªâ match c√°c pattern r·∫•t c·ª• th·ªÉ
    exact_patterns = {
        r'^(hello|hi|hey)$': "hello",
        r'^(thank you|thanks)$': "thanks", 
        r'^(bye|goodbye)$': "bye",
        r'^good (morning|afternoon|evening)$': query_lower,
        r'^how are you\??$': "how are you",
        r'^what\'?s up\??$': "what's up"
    }
    
    for pattern, response_key in exact_patterns.items():
        if re.match(pattern, query_lower):
            return COMMON_RESPONSES.get(response_key, COMMON_RESPONSES.get("hello"))
    
    return None

# Cache-enabled function ƒë·ªÉ l·∫•y categories
def get_all_categories_cached():
    global CATEGORIES_CACHE, CACHE_TIMESTAMP
    current_time = time.time()
    
    # Ki·ªÉm tra cache
    if CATEGORIES_CACHE and (current_time - CACHE_TIMESTAMP) < CACHE_DURATION:
        return CATEGORIES_CACHE
    
    categories = {}
    try:
        docs = db.collection("Category").stream()
        for doc in docs:
            data = doc.to_dict()
            if data and "Name" in data:
                categories[data["Name"]] = data.get("ParentCategory", "None")
        
        # L·ªçc danh m·ª•c cha
        parent_categories = [name for name, parent in categories.items() if parent is None or parent == "None"]
        
        # Update cache
        CATEGORIES_CACHE = sorted(parent_categories)
        CACHE_TIMESTAMP = current_time
        
        return CATEGORIES_CACHE
        
    except Exception as e:
        print(f"‚ùå L·ªói khi l·∫•y danh m·ª•c: {e}")
        return CATEGORIES_CACHE if CATEGORIES_CACHE else []

# Kh·ªüi t·∫°o vectorstore v√† embedding model
print("üöÄ Initializing EvoCasa Assistant...")
embedding = GoogleGenerativeAIEmbeddings(model="models/embedding-001")
vectorstore = Chroma(persist_directory="chroma_store", embedding_function=embedding)

# C√¢n b·∫±ng retriever - ƒë·ªß th√¥ng tin nh∆∞ng kh√¥ng qu√° nhi·ªÅu
retriever = vectorstore.as_retriever(
    search_type="similarity", 
    search_kwargs={"k": 4}  # C√¢n b·∫±ng: ƒë·ªß context nh∆∞ng kh√¥ng qu√° chi ti·∫øt
)

# Hybrid LLM Configuration - Intelligent model selection
def get_llm_for_query(query):
    """Ch·ªçn model ph√π h·ª£p d·ª±a tr√™n ƒë·ªô ph·ª©c t·∫°p c·ªßa c√¢u h·ªèi"""
    query_lower = query.lower()
    
    # Simple queries - use Flash for speed but with better temperature
    simple_patterns = [
        'list', 'show', 'what is', 'how much', 'price', 'cost',
        'available', 'in stock', 'color', 'size', 'dimension'
    ]
    
    # Complex queries - use Pro for better understanding
    complex_patterns = [
        'recommend', 'suggest', 'compare', 'difference', 'better',
        'why', 'how to', 'explain', 'advice', 'opinion', 'return', 'warranty'
    ]
    
    # Check for complex patterns first
    if any(pattern in query_lower for pattern in complex_patterns):
        try:
            return ChatGoogleGenerativeAI(
                model="models/gemini-1.5-pro",
                temperature=0.4,  # TƒÉng ƒë·ªÉ t·ª± nhi√™n h∆°n, √≠t formal
                timeout=20
            ), "Pro"
        except:
            pass
    
    # Check for simple patterns - v·∫´n d√πng Flash nh∆∞ng v·ªõi setting t·ªët h∆°n
    if any(pattern in query_lower for pattern in simple_patterns):
        try:
            return ChatGoogleGenerativeAI(
                model="models/gemini-1.5-flash",
                temperature=0.2, 
                timeout=12
            ), "Flash"
        except:
            pass
    
    # Default to Pro for unknown queries
    try:
        return ChatGoogleGenerativeAI(
            model="models/gemini-1.5-pro",
            temperature=0.3,
            timeout=18
        ), "Pro"
    except ValueError as e:
        print(f"Fallback to Flash due to error: {e}")
        return ChatGoogleGenerativeAI(
            model="models/gemini-1.5-flash",
            temperature=0.2,
            timeout=12
        ), "Flash"

# Initialize default LLM
llm, model_name = get_llm_for_query("default")
print(f"Hybrid LLM system initialized (Default: {model_name})")

# C·∫£i thi·ªán memory buffer - tƒÉng token limit
memory = ConversationBufferMemory(
    memory_key="chat_history",
    return_messages=True,
    output_key="answer",
    input_key="question",
    max_token_limit=2000  # TƒÉng t·ª´ 1000 l√™n 2000 ƒë·ªÉ gi·ªØ ƒë∆∞·ª£c context t·ªët h∆°n
)

# Balanced prompt template - th√¢n thi·ªán nh∆∞ng ng·∫Øn g·ªçn
improved_prompt = PromptTemplate(
    input_variables=["context", "question", "all_categories", "chat_history"],
    template="""You are a friendly EvoCasa furniture consultant. Be helpful, warm, but concise and natural - like talking to a friend who works at a furniture store.

Context: {context}
Categories: {all_categories}
Previous chat: {chat_history}
Question: {question}

RESPONSE STYLE:
- Be conversational and friendly, not formal or robotic
- Give key information clearly without overwhelming details
- Answer the main question first, then add 1-2 helpful extras if relevant
- Keep responses natural length - not too short, not essay-long

FORMATS:
- Prices: "[Product] costs [price]. It's [brief description - style/material]. [One key feature]. Need more details?"
- Returns: "Sure! You have [timeframe] to return items in original condition with receipt. Contact our team at [contact] - they'll walk you through it. What product are you looking to return?"
- General: Answer directly, add context if helpful, offer next steps naturally

If no info available: "I don't have those details right now, but our team at support@evocasa.com or 1800-XXX-XXX can help you out! Anything else I can check for you?"

Keep it natural and helpful - like a good salesperson, not a manual!

Answer:"""
)

# Dynamic chain creation function
def create_qa_chain(llm_instance):
    """T·∫°o chain v·ªõi LLM ƒë∆∞·ª£c ch·ªâ ƒë·ªãnh"""
    return ConversationalRetrievalChain.from_llm(
        llm=llm_instance,
        retriever=retriever,
        memory=memory,
        chain_type="stuff",
        return_source_documents=False,
        output_key="answer",
        combine_docs_chain_kwargs={"prompt": improved_prompt}
    )

# Initialize default chain
qa_chain = create_qa_chain(llm)

# Pre-load categories khi kh·ªüi ƒë·ªông
print("üìã Pre-loading categories...")
categories_preload = get_all_categories_cached()
categories_text = "\n‚Ä¢ ".join(categories_preload) if categories_preload else "No categories found"

# Welcome message
print("=" * 70)
print("üè† EVOCASA ASSISTANT - BALANCED & FRIENDLY üòä")
print("=" * 60)
print("Perfect balance features:")
print("‚Ä¢ üí¨ Natural, conversational responses")
print("‚Ä¢ üéØ Key information without overwhelming details") 
print("‚Ä¢ ü§ñ Smart model selection (Flash/Pro)")
print("‚Ä¢ ‚ö° Fast response with quality answers")
print("‚Ä¢ üìö Right amount of context (4 documents)")
print("‚Ä¢ üß† Good conversation memory")
print("-" * 70)
print("üí¨ Ask me anything about EvoCasa furniture! (type 'exit' to quit)")
print("-" * 70)

# Performance tracking
query_count = 0
total_response_time = 0

while True:
    query = input("\nüôã You: ").strip()
    
    if query.lower() == "exit":
        print("\nü§ñ EvoCasa Assistant: Thank you so much for visiting EvoCasa today! üåü")
        print("It was wonderful helping you explore our furniture collection.")
        print("Remember, I'm always here whenever you need assistance with your home furnishing needs.")
        if query_count > 0:
            avg_response_time = total_response_time / query_count
            print(f"üìä Session Summary: {query_count} questions answered, average response time: {avg_response_time:.2f}s")
        print("Have a fantastic day! üëã")
        break
    
    if not query:
        print("\nü§ñ EvoCasa Assistant: I'm here and ready to help! üòä Please feel free to ask me anything about our furniture collection, pricing, policies, or let me know how I can assist you today!")
        continue
    
    start_time = time.time()
    query_count += 1
    
    # Check for common responses first
    common_response = check_common_response(query)
    if common_response:
        response_time = time.time() - start_time
        total_response_time += response_time
        print(f"\nü§ñ EvoCasa Assistant: {common_response}")
        continue
    
    # Show loading message
    print("\nü§ñ Analyzing your question and gathering information... ‚è≥", end="", flush=True)
    
    try:
        # Intelligent model selection based on query complexity
        selected_llm, model_type = get_llm_for_query(query)
        
        # Create appropriate chain for this query
        current_chain = create_qa_chain(selected_llm)
        
        # Update loading message with model info
        print(f"\rü§ñ Processing with {model_type} model... ‚è≥", end="", flush=True)
        
        # Execute query
        result = current_chain.invoke({
            "question": query, 
            "all_categories": categories_text
        })
        
        response_time = time.time() - start_time
        total_response_time += response_time
        
        # Clear loading message and show result
        print(f"\rü§ñ EvoCasa Assistant:")
        print(f"{result['answer']}")
        
        # Performance indicator (ch·ªâ hi·ªÉn th·ªã n·∫øu ch·∫≠m)
        if response_time > 10:
            print(f"‚è±Ô∏è Response time: {response_time:.1f}s")
            
    except Exception as e:
        response_time = time.time() - start_time
        total_response_time += response_time
        print(f"\rü§ñ EvoCasa Assistant: I apologize, but I'm experiencing some technical difficulties right now. üòî")
        print("For immediate assistance, please contact our customer service team:")
        print("üìß Email: support@evocasa.com")
        print("üìû Phone: 1800-XXX-XXX")
        print("I'll be back to full functionality shortly. Thank you for your patience!")
        
        if "timeout" in str(e).lower():
            print("‚è±Ô∏è The request took longer than expected - our team can provide faster assistance via phone or email.")

print("\nüè† Thank you for choosing EvoCasa - where your home dreams come true! üåü")